<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>2023 Capstone Project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>DSC</strong> <span>180</span></a>
					</header>



				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
						<section id="one">
							<div class="inner">
								<header class="major">
									<h1>Investigation of Adversarial Attack on GNN</h1>
									<h3>Attack Graph Classification Perturbation in Latent Representation of Graphs </h3>
									<h4>Winston Yu, Jianming Geng, Barry Xue</h4>
									<h4>UCSD Halıcıoğlu Data Science Institute</h4>
									<h5>UC San Diego Data Science Senior Capstone Project</h5>
								</header>
								<div class="image section">
									<!-- Thinking about switching this to another image -->
									<span class="image main"><a href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf" traget="_blank"><img src="images/GraphRNN_Result_Official.png" alt="GraphRNN Results" /></a></span>
									<p>Figure 1: From "GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models"; Comparison between original graph (top row), generation from GraphRNN, a deep auto-regressive graph generation model (second row), and random generation baseline (bottom row). </p>
								</div>
							</div>
						</section>

						<section id="two">
							<div class="inner">
								<header class="major">
									<h3>Introduction</h3>
								</header>
								<!-- Write the abstract here -->
								<p>Graph neural networks (GNNs) have seen tremendous success in recent years, leading to their widespread adoption in a variety of applications. However, recent studies have demonstrated that GNNs can be vulnerable to adversarial attacks. Adversarial attacks allow adversaries to manipulate GNNs by introducing small, seemingly harmless perturbations to the input data, which can lead to misclassification or unexpected behavior from the GNN. Such attacks can have drastic impacts on security and privacy, particularly in safety-critical or privacy-sensitive applications. Consequently, research into adversarial attacks on GNNs is becoming increasingly important, and we thus propose an adversarial attack framework (called Adversarial WGraphRNN) for Graph Classification problems. Specifically, our framework generates more natural attacks on the latent space of an arbitrarily benign graph. Furthermore, instead of manually generating attacks, the framework also employs deep learning techniques to automate the attack process. In this project, we identify vulnerabilities in current graph classifiers and provide motivation for researchers to discover more robust models. </p>
							</div>
						</section>

						<!-- Write our referenced resources here -->
						<section id="three">
							<div class="inner">
								<header class="major">
									<h3>Background</h3>
								</header>
								<h5>1. Generating Attack with Surrogate Model</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/abs/2111.02842" target="_blank"><img src="images/overall_grabnel.png" alt="" /></a></span>
									<p>Figure 2: From "Adversarial Attacks on Graph Classification via Bayesian Optimisation"; a visualization of the attack generation procedure.</p>
								</div>
								<p>Generating Attack with Surrogate Model on Graphs is a method used to find the most effective attacks on graphs using machine learning algorithms. The main goal is to learn a surrogate model from the data of the graph, and then use the generated model to simulate attack scenarios and search for the most effective attack strategy. This technique can be especially useful for cyber-security applications, such as identifying potential weaknesses in networks or finding malicious pathways in a graph.</p>
								<h5>2. Generating Adversarial Example with WGAN</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Natural_Adversarial_Example.png" alt="" /></a></span>
									<p>Figure 3: From "Generating Natural Adversarial Example"; A visualization of the Wasserstein GAN procedure. </p>
								</div>
								<p>This approach explores a novel method for generating natural adversarial examples, which are inputs that can fool state-of-the-art machine learning models while being imperceptible to human observation. It proposes an approach which combines generative adversarial networks (GANs) with an adversarial perturbation technique to generate both semantically and naturally valid adversarial examples. Specifically, it introduces the Invertor-Generator architecture to train the generative models and uses a Search Algorithm to generate the attack. Inverter is a technique that uses a trained model to transform given data into laten space. Generator is a technique that seeks to directly generate natural adversarial examples from data in the latent space. The critic so-called WGAN is a measurement of difference between the true input and the fake input (generated). </p>
								<h5>3. Graph2Vec</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Graph2Vec Illustration.jpeg" alt="" /></a></span>
									<p>Figure 4: From "Generating Natural Adversarial Example"; A visualization of the Graph2Vec structure. </p>
								</div>
								<p>Graph2Vec is a machine learning method developed to embed any graph of arbitrary size into a high-dimensional vector space. It leverages the structural information present in a graph, such as the node degree, shortest path length, etc. to learn a feature vector representation of each node within the graph. By doing so, Graph2Vec can be used to compare the similarity between graphs of different sizes and types. Thus, it provides a meaningful measurement to graph-based data.</p>
							</div>
						</section>

						<!-- Write our component here  -->
						<section id="four">
							<div class="inner">
								<header class="major">
									<h4>Our Proposal</h4>
								</header>
								<p>
									<strong>Discriminator: MLP</strong> <br><br>
									<strong>Generator: GraphRNN</strong> <br><br>
									<strong>Inverter: Graph2Vec + MLP</strong> <br><br>
								</p>

								<header class="style4">
									<h4>Generator: GraphRNN</h4>
									<div class="image section">
										<span class="image main"><a href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf" target="_blank"><img src="images/GraphRNN_Structure.png" alt="" /></a></span>
										<p>Figure 4: Structure of the GraphRNN model. </p>
									</div>
									<p>
										GraphRNN is an autoregressive graph generation model that uses two GRU recurrent neural networks. One GRU model update the node-level sequence, which decides whether to generate a new node at a given iteration.
										The second GRU model update the edge-level sequence after the node-level sequence is updated; it will fill in the connection between existing nodes and the next iteration of node.
									</p>
									<p>
										Originally, the model is trained with binary cross entropy loss, comparing the generated edges and the ground truth. We are training the generator in as a WGAN generator, meaning we are training GraphRNN to maximize the discriminator's output for fake samples.
									</p>
								</header>

								<header class="style4">
									<h4>Inverter&Discriminator: Multi-layer Neural Network</h4>
									<p>
										The inverter and the discriminator are simple multi-layer perceptron networks. The difference lies in their hyperparameter setup, and that the inverter takes in the embedding of a graph.
									</p>
									<p>
										For the inverter, we decided to use Graph2Vec to generate embedding for the real and the generated graph, and pass that into the inverter for training.
									</p>
								</header>

								<header class="style4">
									<h4>Overall Structure</h4>
									<div class="image section">
										<span class="image main"><a href="" target="_blank"><img src="images/Method Illustraion.png" alt="" /></a></span>
										<p>Figure 5.1: Structure of our attack generation procedure. </p>
									</div>
									<p>
										We divide our training algorithm into four parts in total: (1). Discriminator; (2). Generator; (3). Inverter; (4). Search Algorithm.
									</p>
									<p>
										We first train the Discriminator, which corresponds to dark blue from the image. Discriminator takes in two inputs: the original graph in the form of an adjacency matrix and the and the fake graph from Generator. In order to complete the sole training of discriminator, we here turn off the training of the Generator and only leave the training of the Discriminator on. We use Wasserstein Loss to effectively compare the difference of the two graph.
									</p>
									<p>
										We then train the Generator, which corresponds to the dark green from the image. The training of the Generator is rather straightforward. We first sample noises from a normal distribution and pass it into the GraphRNN to generate the fake graph. Without loss of generality, we use Discriminator as our critic during the training of Generator.
									</p>
									<p>
										After that, we train the Inverter, which corresponds to the red blocks from the image. The logistics of trianing Inverter is not much different from training Generator, except that we are working backwards now. We first pass a graph input into Graph2Vec to generate some embeddings for the purpose of size standardization. Now the vectorized data should have a uniform shape, as against a non-constant shape before Graph2Vec. We then pass this vectorized graph into Inverter and subsequently Generator to generate a reconstructed graph. At the same time, we also generate noise from Guassian Distribution and pass this noise into the Graph2vec-Inverter pipeline to similarly generate a reconstructed noise. Finally, we compare the true graph, reconstructed graph, true noise, and reconstructed noise to update the weights of Inverter.
									</p>
								</header>

								<header class="style4">
									<h4>What we have currently</h4>
									<div class="image section">
										<span class="image main"><a href="" target="_blank"><img src="images/Method Illustraion2.png" alt="" /></a></span>
										<p>Figure 5.2: Current structure of our attack generation procedure. </p>
									</div>
								</header>
							</div>
						</section>

						<!-- Here is our results, probably not empirical results, but the training procedure -->
						<section id="five">
							<div class="inner">
								<header class="major">
									<h4>Result of our Investigation</h4>
								</header>
								<h5>1. New Training Procedure for GraphRNN</h5>
								<p>
									Originally, GraphRNN is trained by comparing the generated padded adjacency vector with the sequenced original adjacency matrix using binary cross entropy loss.
									This isn't ideal for the WGAN training procedure, where the generator needs to be trained by the discriminator's output, which is no longer an adjacency matrix.
								</p>

								<p>
									For our new training process for the GraphRNN, we revert the output of node-level and edge-level RNN models to batches of generated adjacency matrices; then we pass these
									adjacency matrices into the discriminator network, whose output is used in our Wasserstein loss function. We will use this loss to update the node-level and edge-level RNN model parameters.
								</p>

								<!-- This is prob going to be a discussion of how our three losses are not working -->
								<h5>2. Observation on the three Losses</h5>
								<p>
									With our proposed WGAN training regime, we trained the generator, the discriminator, and the inverter, all in one training loop.
									We recorded the loss of each of the models every epoch.
								</p>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/pic11.jpg" alt="" /></a></span>
									<p>Figure 6: Graphs for the training loss of the three models. </p>
								</div>
								<p>
									We found that as the discriminator loss improves overtime, the generator loss and inverter loss quickly hits a threshold and cannot
									improve further beyond this point.

									This can be an indication that the new training regime isn't suitable for GraphRNN training, and the quality of our generated graphs is not good enough
									support the training of the discriminator and inverter.
								</p>
							</div>
						</section>

						<!-- Here we write about what went wrong, and some ideas to make them right -->
						<section id="six">
							<div class="inner">
								<header class="major">
									<h4>Challenges</h4>
								</header>
								<h5>1. Previous work and time constraint</h5>
								<p>
									There aren't many research done in generating adversarial examples in latent space for graph classification task,
									meaning we have to compose our idea from the ground up, which is a huge undertaking. Plus, the time frame for this
									project is 10 weeks, which is very little time to fully develop this framework and run experiments.
								</p>

								<h5>2. GraphRNN output during training is difficult to interpret and process by the discriminator </h5>
								<p>
									Theoretically, the output of GraphRNN is a graph of arbitrary size. In practice, however, the training implementation output
									packed version of the original graph adjacency vector in batches. Ideally, we want to extract graph statistics from the graph adjacency
									matrix and use those extracted statistics for classification in the discriminator. This is not possible with the packed output.
									Although we are able to reshape the packed output back into the expected shape of a batch of adjacency matrices, the validity of this operation is still unconfirmed.
								</p>

								<h5>3. Lack of networkx integration in PyTorch/PyTorch Geometric</h5>

								<p>
									We can't calculate graph statistics between models forward passes, because of the lack of Networkx function support from PyTorch Geometric.
								</p>
								<ul>
									<li>
										<p>
										The procedure to extract high-quality graph features (clustering coefficient, for example) is complex, and we have been relying on Networkx functions to do these calculations.
										This becomes an issue during the training of the GraphRNN because Networkx functions break the gradient propagation map in PyTorch, causing the model parameters to not update. We resort
										to a simple MLP for the discriminator that takes the adjacency matrix directly, but this model has less predictive power than our original proposed model.
										</p>
									</li>
								</ul>


								<header class="major">
									<h4>Future Plan</h4>
								</header>
								<p>If we plan to continue this research project, we want to do the following...</p>
								<h5>1. Improve implementation of GraphRNN for better training procedure</h5>
								<p>
									Due to the lack of time, we didn't investigate the GraphRNN implementation and treated the model as a black box.
                                    We encountered many issues related to dataloader, model training, and graph generation with the official implementation. As we start to troubleshooting
                                    the model, we noticed that the training code isn't optimal for WGAN training, because its input and output are sequenced adjacency vectors, instead we want complete adjacency matrices for down stream training and evaluation.
                                    We want to redesign and reimplement the training code to return graphs' adjacency matrix in one forward pass, and be ready for WGAN-style training.
								</p>
								<h5>2. Use graph classification model as our discriminator, instead of a simple MLP</h5>
								<p>
									Currently, the discriminator is a simple MLP that takes in the real/generated adjacency matrix and output a classification result. The discriminator is trained by the negative Wasserstein distance between the generated distribution
									and the real data distribution.
								</p>
								<p>
									After evaluating the result of the current setup, we think a simple MLP can't take into account specific graph properties, and thus cannot accurately discern between the two distributions.
									We propose to substitute this MLP with a Graph Attention Machine (GAM) classifier, because GAM can utilize the Weisfeiler-Lehman kernel to capture graph features to discern between the real and the generated distributions.
									Ideally, this will provide high quality loss for the discriminator training and the generator training.
								</p>
							</div>
						</section>



						<section id="eight">
							<div class="inner">
								<header class="major">
									<h4>Reference</h4>
								</header>
								<ol>
									<li>GraphRNN: <a href="https://arxiv.org/abs/1802.08773" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/snap-stanford/GraphRNN">Original Implementation</a></li>
									<li>Attack on GNN with Bayesian Optimization: <a href="https://arxiv.org/abs/2111.02842" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/xingchenwan/grabnel">Original Implementation</a></li>
									<li>Wasserstein GAN: <a href="https://arxiv.org/abs/1701.07875" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/martinarjovsky/WassersteinGAN">Original Implementation</a></li>
									<li>Graph Attention Model: <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Paper</a>, <a target="_blank" href="https://github.com/benedekrozemberczki/GAM/tree/03e13f01a59dba22c9c789dfa11ce84b614c0be0">Original Implementation</a></li>
								</ol>
							</div>
						</section>
					</div>

				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<!-- TODO: Replace with actual Email and LinkedIn links -->
										<h3>Winston Yu</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
										<h3>Jianming Geng</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
										<h3>Barry Xue</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
									</div>
								</section>
							</section>
							<section class="split">
								<div>
									<p> Project GitHub Page</p>
									<a href="https://github.com/Barry0121/graph-neural-net-benchmark" target="_blank" rel="noopener noreferrer" class="icon brands alt fa-github"><span class="label">Project GitHub Page</span></a>
								</div>
							</section>
						</div>
					</section>


				<!-- Footer -->
					<!-- <footer id="footer">
						<div class="inner">
							<ul class="icons">

							</ul>
						</div>
					</footer> -->

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>