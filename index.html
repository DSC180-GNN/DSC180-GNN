<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>2023 Capstone Project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>DSC</strong> <span>180</span></a>
					</header>



				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
						<section id="one">
							<div class="inner">
								<header class="major">
									<h1>Discovering Continuous Latent Space Representations for Graphs</h1>
									<!-- <h3>Attack Graph Classification Perturbation in Latent Representation of Graphs </h3> -->
									<h4>Winston Yu, Jianming Geng, Barry Xue</h4>
									<h4>UCSD Halıcıoğlu Data Science Institute</h4>
									<h5>UC San Diego Data Science Senior Capstone Project</h5>
								</header>
								<div class="image section">
									<!-- Thinking about switching this to another image -->
									<span class="image main"><a href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf" traget="_blank"><img src="images/GraphRNN_Result_Official.png" alt="GraphRNN Results" /></a></span>
									<p>Figure 1: From "GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models"; Comparison between original graph (top row), generation from GraphRNN, a deep auto-regressive graph generation model (second row), and random generation baseline (bottom row). </p>
								</div>
							</div>
						</section>

						<section id="two">
							<div class="inner">
								<header class="major">
									<h3>Introduction</h3>
								</header>
								<!-- Write the abstract here -->
								<p>
									Graph neural networks (GNNs) have seen tremendous success in recent years, leading to their widespread adoption
									in a variety of applications. However, recent studies have demonstrated that GNNs can be vulnerable to adversarial
									attacks. Adversarial attacks allow adversaries to manipulate GNNs by introducing small, seemingly harmless perturbations
									to the input data, which can lead to misclassification or unexpected behavior from the GNN. Such attacks can have drastic
									impacts on security and privacy, particularly in safety-critical or privacy-sensitive applications.
									Consequently, research into adversarial attacks on GNNs is becoming increasingly important,
									and we thus propose an adversarial attack framework that discovers continuous latent representations
									of graphs to which adversarial modifications are added, instead of modifying a benign graph itself.
								</p>
							</div>
						</section>

						<!-- Write our referenced resources here -->
						<section id="three">
							<div class="inner">
								<header class="major">
									<h3>Background</h3>
								</header>
								<h5>1. Generating Attack with Surrogate Model</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/abs/2111.02842" target="_blank"><img src="images/overall_grabnel.png" alt="" /></a></span>
									<p>Figure 2: From "Adversarial Attacks on Graph Classification via Bayesian Optimisation"; a visualization of the attack generation procedure.</p>
								</div>
								<p>Generating Attack with Surrogate Model on Graphs is a method used to find the most effective attacks on graphs using machine learning algorithms. The main goal is to learn a surrogate model from the data of the graph, and then use the generated model to simulate attack scenarios and search for the most effective attack strategy. This technique can be especially useful for cyber-security applications, such as identifying potential weaknesses in networks or finding malicious pathways in a graph.</p>
								<h5>2. Generating Adversarial Example with WGAN</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Natural_Adversarial_Example.png" alt="" /></a></span>
									<p>Figure 3: From "Generating Natural Adversarial Example"; A visualization of the Wasserstein GAN procedure. </p>
								</div>
								<p>
									This approach is a novel method for generating natural adversarial examples,
									which are inputs that can fool state-of-the-art machine learning models
									while being imperceptible to human observation.
									Its architecture is comprised of an inverter, which maps from the data space to the latent space,
									a generator, which is trained to be part of a Wasserstein GAN and maps from the latent space to the data space,
									and a search algorithm, which explores the latent space for representations of adversarial examples.
								</p>
								<h5>3. Graph2Vec</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Graph2Vec Illustration.jpeg" alt="" /></a></span>
									<p>Figure 4: From "Generating Natural Adversarial Example"; A visualization of the Graph2Vec structure. </p>
								</div>
								<p>
									Graph2Vec is a machine learning method that embeds a graph of arbitrary size into Euclidean space.
									We intend to first pass graphs to Graph2Vec and then pass the resulting vectors to the inverter.
								</p>
							</div>
						</section>

						<!-- Write our component here  -->
						<section id="four">
							<div class="inner">
								<header class="major">
									<h4>Our Proposal</h4>
								</header>
								<p>
									<strong>Discriminator: MLP</strong> <br><br>
									<strong>Generator: GraphRNN</strong> <br><br>
									<strong>Inverter: Graph2Vec/GAM + MLP</strong> <br><br>
								</p>

								<header class="style4">
									<h4>Generator: GraphRNN</h4>
									<div class="image section">
										<span class="image main"><a href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf" target="_blank"><img src="images/GraphRNN_Structure.png" alt="" /></a></span>
										<p>Figure 4: Structure of the GraphRNN model. </p>
									</div>
									<p>
										GraphRNN is an autoregressive graph generation model that uses two GRU recurrent neural networks.
										The first GRU model updates a hidden state, which is a representation of the graph that has been generated so far.
										The second GRU model takes the hidden state of the first GRU and uses it to generate adjacency vectors.
										Originally, the model was trained with binary cross entropy loss, comparing the generated edges and the ground truth.
										We propose to train the GraphRNN to minimize the Wasserstein distance between the data distribution and the distribution of GraphRNN.
									</p>
								</header>

								<header class="style4">
									<h4>Inverter & Discriminator: Multi-layer Perceptrons</h4>
									<p>
										The inverter and the discriminator are simple multi-layer perceptrons.
										For the inverter, we decided to first try to train Graph2Vec to generate embeddings, which would be passed to the inverter.
										However, we later found that the Graph2Vec implementation from the package “Karate Club” is not compatible with Torch’s gradient computation.
										Hence, we decided to switch to GAM (Graph Attention Model) as an embedding technique.
										However, GAM did not work either.
									</p>
								</header>

								<header class="style4">
									<h4>Overall Planned Structure</h4>
									<div class="image section">
										<span class="image main"><a href="" target="_blank"><img src="images/Method Illustration.png" alt="" /></a></span>
										<p>Figure 5.1: Structure of our attack generation procedure. </p>
									</div>
									<p>
										We divide our training algorithm into four parts in total: (1). Discriminator; (2). Generator; (3). Inverter; (4). Search Algorithm.
									</p>
									<p>
										We first train the Discriminator, which corresponds to dark blue from the image. Discriminator takes in two inputs: the original graph in the form of an adjacency matrix and the and the fake graph from Generator. In order to complete the sole training of discriminator,
										we here turn off the training of Generator and only leave the training of the Discriminator on. We use Wasserstein Loss to effectively compare the difference of the two graph.
									</p>
									<p>
										We then train the Generator, which corresponds to the dark green from the image. The training of the Generator is rather straightforward. We first sample noises from a normal distribution and pass it into the GraphRNN to generate the fake graph. Without loss of generality,
										we use Discriminator as our critic during the training of Generator.
									</p>
									<p>
										After that, we train the Inverter, which corresponds to the red blocks from the image. The logistics of trianing Inverter is not much different from training Generator, except that we are using a different architecture and dealing with data of different sizes. We first
										pass a graph input into Graph2Vec (or GAM) to generate some embeddings for the purpose of graph standardization. Before we pass graphs (as in the form of an adjacency matrix), those adj matrices have various sizes, which make the training of MLP harder. Hence, we
										transform those adj matrices into an embedding representation (vector form) to unify their sizes. Now the vectorized data should have a uniform shape, as against a non-constant shape before Graph2Vec/GAM. We then pass this vectorized graph into the MLP and subsequently
										Generator to generate a reconstructed graph. At the same time, we also generate noise from Guassian Distribution and pass this noise into the Graph2vec-MLP pipeline similarly to generate a reconstructed noise. Finally, we compare the true graph, reconstructed graph,
										true noise, and reconstructed noise to update the weights of Inverter.
									</p>
								</header>

								<header class="style4">
									<h4>What We Have Currently</h4>
									<div class="image section">
										<span class="image main"><a href="" target="_blank"><img src="images/Method Illustraion2.png" alt="" /></a></span>
										<p>Figure 5.2: Current structure of our attack generation procedure. </p>
									</div>
									<p>
										We can only get to Discriminator and Generator training due to time constraint.
									</p>
								</header>
							</div>
						</section>

						<!-- Here is our results, probably not empirical results, but the training procedure -->
						<section id="five">
							<div class="inner">
								<header class="major">
									<h4>Result of our Investigation</h4>
								</header>
								<h5>1. New Training Procedure for GraphRNN</h5>
								<p>
									Originally, GraphRNN is trained by comparing the generated padded adjacency vector with the sequenced original adjacency matrix using binary cross entropy loss.
									This isn't ideal for the WGAN training procedure, where the generator needs to be trained by the discriminator's output, which is no longer an adjacency matrix.
								</p>

								<p>
									For our new training process for the GraphRNN, we revert the output of node-level and edge-level RNN models to batches of generated adjacency matrices; then we pass these
									adjacency matrices into the discriminator network, whose output is used in our Wasserstein loss function. We will use this loss to update the node-level and edge-level RNN model parameters.
								</p>

								<!-- This is prob going to be a discussion of how our three losses are not working -->
								<h5>2. Observation on the three Losses</h5>
								<p>
									With our proposed WGAN training regime, we trained the generator, the discriminator, and the inverter, all in one training loop.
									We recorded the loss of each of the models every epoch.
								</p>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Graph Embedding.png" alt="" /></a></span>
									<p>Figure 6: Comparison of Graph2Vec Embedding between Real Graphs and Generated Graphs. </p>
								</div>
								<p>
									We found that as the discriminator loss improves overtime, the generator loss and inverter loss quickly hits a threshold and cannot
									improve further beyond this point.

									This can be an indication that the new training regime isn't suitable for GraphRNN training, and the quality of our generated graphs is not good enough
									support the training of the discriminator and inverter.
								</p>
							</div>
						</section>

						<!-- Here we write about what went wrong, and some ideas to make them right -->
						<section id="six">
							<div class="inner">
								<header class="major">
									<h4>Challenges</h4>
								</header>
								<h5>1. Previous work and time constraint</h5>
								<p>
									There aren't many research done in generating adversarial examples in latent space for graph classification task,
									meaning we have to compose our idea from the ground up, which is a huge undertaking. Plus, the time frame for this
									project is 10 weeks, which is very little time to fully develop this framework and run experiments.
								</p>

								<h5>2. GraphRNN output during training is difficult to interpret and process by the discriminator </h5>
								<p>
									Theoretically, the output of GraphRNN is a graph of arbitrary size. In practice, however, the training implementation output
									packed version of the original graph adjacency vector in batches. Ideally, we want to extract graph statistics from the graph adjacency
									matrix and use those extracted statistics for classification in the discriminator. This is not possible with the packed output.
									Although we are able to reshape the packed output back into the expected shape of a batch of adjacency matrices, the validity of this operation is still unconfirmed.
								</p>

								<h5>3. Lack of networkx integration in PyTorch/PyTorch Geometric</h5>

								<p>
									The procedure to extract high-quality graph features (clustering coefficient, for example) is complex, and we have been relying on Networkx functions to do these calculations.
										This becomes an issue during the training of the GraphRNN because Networkx functions break the gradient propagation map in PyTorch, causing the model parameters to not update. We resort
										to a simple MLP for the discriminator that takes the adjacency matrix directly, but this model has less predictive power than our original proposed model.
								</p>

								<header class="major">
									<h4>Future Plan</h4>
								</header>
								<p>If we plan to continue this research project, we want to do the following...</p>
								<h5>1. Improve implementation of GraphRNN for better training procedure</h5>
								<p>
									Due to the lack of time, we didn't investigate the GraphRNN implementation and treated the model as a black box.
                                    We encountered many issues related to dataloader, model training, and graph generation with the official implementation. As we start to troubleshooting
                                    the model, we noticed that the training code isn't optimal for WGAN training, because its input and output are sequenced adjacency vectors, instead we want complete adjacency matrices for down stream training and evaluation.
                                    We want to redesign and reimplement the training code to return graphs' adjacency matrix in one forward pass, and be ready for WGAN-style training.
								</p>
								<h5>2. Use graph classification model as our discriminator, instead of a simple MLP</h5>
								<p>
									Currently, the discriminator is a simple MLP that takes in the real/generated adjacency matrix and output a classification result. The discriminator is trained by the negative Wasserstein distance between the generated distribution
									and the real data distribution.
								</p>
								<p>
									After evaluating the result of the current setup, we think a simple MLP can't take into account specific graph properties, and thus cannot accurately discern between the two distributions.
									We propose to substitute this MLP with a Graph Attention Machine (GAM) classifier, because GAM can utilize the Weisfeiler-Lehman kernel to capture graph features to discern between the real and the generated distributions.
									Ideally, this will provide high quality loss for the discriminator training and the generator training.
								</p>
							</div>
						</section>



						<section id="eight">
							<div class="inner">
								<header class="major">
									<h4>Reference</h4>
								</header>
								<ol>
									<li>GraphRNN: <a href="https://arxiv.org/abs/1802.08773" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/snap-stanford/GraphRNN">Original Implementation</a></li>
									<li>Attack on GNN with Bayesian Optimization: <a href="https://arxiv.org/abs/2111.02842" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/xingchenwan/grabnel">Original Implementation</a></li>
									<li>Wasserstein GAN: <a href="https://arxiv.org/abs/1701.07875" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/martinarjovsky/WassersteinGAN">Original Implementation</a></li>
									<li>Graph Attention Model: <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Paper</a>, <a target="_blank" href="https://github.com/benedekrozemberczki/GAM/tree/03e13f01a59dba22c9c789dfa11ce84b614c0be0">Original Implementation</a></li>
								</ol>
							</div>
						</section>
					</div>

				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<!-- TODO: Replace with actual Email and LinkedIn links -->
										<h3>Winston Yu</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
										<h3>Jianming Geng</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
										<h3>Barry Xue</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
									</div>
								</section>
							</section>
							<section class="split">
								<div>
									<p> Project GitHub Page</p>
									<a href="https://github.com/Barry0121/graph-neural-net-benchmark" target="_blank" rel="noopener noreferrer" class="icon brands alt fa-github"><span class="label">Project GitHub Page</span></a>
								</div>
							</section>
						</div>
					</section>


				<!-- Footer -->
					<!-- <footer id="footer">
						<div class="inner">
							<ul class="icons">

							</ul>
						</div>
					</footer> -->

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>