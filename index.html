<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>2023 Capstone Project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>DSC</strong> <span>180</span></a>
					</header>



				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
						<section id="one">
							<div class="inner">
								<header class="major">
									<h1>Discovering Continuous Latent Space Representations for Graphs</h1>
									<!-- <h3>Attack Graph Classification Perturbation in Latent Representation of Graphs </h3> -->
									<h4>Winston Yu, Jianming Geng, Barry Xue</h4>
									<h4>UCSD Halıcıoğlu Data Science Institute</h4>
									<h5>UC San Diego Data Science Senior Capstone Project</h5>
								</header>
								<div class="image section">
									<!-- Thinking about switching this to another image -->
									<span class="image main"><a href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf" traget="_blank"><img src="images/GraphRNN_Result_Official.png" alt="GraphRNN Results" /></a></span>
									<p>Figure 1: From "GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models"; Comparison between original graph (top row), generation from GraphRNN, a deep auto-regressive graph generation model (second row), and random generation baseline (bottom row). </p>
								</div>
							</div>
						</section>

						<section id="two">
							<div class="inner">
								<header class="major">
									<h3>Introduction</h3>
								</header>
								<!-- Write the abstract here -->
								<p>
									Graph neural networks (GNNs) have seen tremendous success in recent years, leading to their widespread adoption
									in a variety of applications. However, recent studies have demonstrated that GNNs can be vulnerable to adversarial
									attacks. Adversarial attacks allow adversaries to manipulate GNNs by introducing small, seemingly harmless perturbations
									to the input data, which can lead to misclassification or unexpected behavior from the GNN. Such attacks can have drastic
									impacts on security and privacy, particularly in safety-critical or privacy-sensitive applications.
									Consequently, research into adversarial attacks on GNNs is becoming increasingly important,
									and we thus propose an adversarial attack framework that discovers continuous latent representations
									of graphs to which adversarial modifications are added, instead of modifying a benign graph itself.
								</p>
							</div>
						</section>

						<!-- Write our referenced resources here -->
						<section id="three">
							<div class="inner">
								<header class="major">
									<h3>Background</h3>
								</header>
								<h5>1. Generating Attack with Surrogate Model</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/abs/2111.02842" target="_blank"><img src="images/overall_grabnel.png" alt="" /></a></span>
									<p>Figure 2: From "Adversarial Attacks on Graph Classification via Bayesian Optimisation"; a visualization of the attack generation procedure.</p>
								</div>
								<p>Generating Attack with Surrogate Model on Graphs is a method used to find the most effective attacks on graphs using machine learning algorithms. The main goal is to learn a surrogate model from the data of the graph, and then use the generated model to simulate attack scenarios and search for the most effective attack strategy. This technique can be especially useful for cyber-security applications, such as identifying potential weaknesses in networks or finding malicious pathways in a graph.</p>
								<h5>2. Generating Adversarial Example with WGAN</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Natural_Adversarial_Example.png" alt="" /></a></span>
									<p>Figure 3: From "Generating Natural Adversarial Example"; A visualization of the Wasserstein GAN procedure. </p>
								</div>
								<p>
									This approach is a novel method for generating natural adversarial examples,
									which are inputs that can fool state-of-the-art machine learning models
									while being imperceptible to human observation.
									Its architecture is comprised of an inverter, which maps from the data space to the latent space,
									a generator, which is trained to be part of a Wasserstein GAN and maps from the latent space to the data space,
									and a search algorithm, which explores the latent space for representations of adversarial examples.
								</p>
								<h5>3. Graph2Vec</h5>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Graph2Vec Illustration.jpeg" alt="" /></a></span>
									<p>Figure 4: From "Generating Natural Adversarial Example"; A visualization of the Graph2Vec structure. </p>
								</div>
								<p>
									Graph2Vec is a machine learning method that embeds a graph of arbitrary size into Euclidean space.
									We intend to first pass graphs to Graph2Vec and then pass the resulting vectors to the inverter.
								</p>
							</div>
						</section>

						<!-- Write our component here  -->
						<section id="four">
							<div class="inner">
								<header class="major">
									<h4>Our Proposal</h4>
								</header>
								<p>
									<strong>Discriminator: MLP</strong> <br><br>
									<strong>Generator: GraphRNN</strong> <br><br>
									<strong>Inverter: Graph2Vec/GAM + MLP</strong> <br><br>
								</p>

								<header class="style4">
									<h4>Generator: GraphRNN</h4>
									<div class="image section">
										<span class="image main"><a href="https://cs.stanford.edu/people/jure/pubs/graphrnn-icml18.pdf" target="_blank"><img src="images/GraphRNN_Structure.png" alt="" /></a></span>
										<p>Figure 4: Structure of the GraphRNN model. </p>
									</div>
									<p>
										GraphRNN is an autoregressive graph generation model that uses two GRU recurrent neural networks.
										The first GRU model updates a hidden state, which is a representation of the graph that has been generated so far.
										The second GRU model takes the hidden state of the first GRU and uses it to generate adjacency vectors.
										Originally, the model was trained with binary cross entropy loss, comparing the generated edges and the ground truth.
										We propose to train the GraphRNN to minimize the Wasserstein distance between the data distribution and the distribution of GraphRNN.
									</p>
								</header>

								<header class="style4">
									<h4>Inverter & Discriminator: Multi-layer Perceptrons</h4>
									<p>
										The inverter and the discriminator are simple multi-layer perceptrons.
										For the inverter, we decided to first try to train Graph2Vec to generate embeddings, which would be passed to the inverter.
										However, we later found that the Graph2Vec implementation from the package “Karate Club” is not compatible with Torch’s gradient computation.
										Hence, we decided to switch to GAM (Graph Attention Model) as an embedding technique.
										However, GAM did not work either.
									</p>
								</header>

								<header class="style4">
									<h4>Overall Planned Structure</h4>
									<div class="image section">
										<span class="image main"><a href="" target="_blank"><img src="images/Method Illustration.png" alt="" /></a></span>
										<p>Figure 5.1: Structure of our attack generation procedure. </p>
									</div>
									<p>
										We divide our training algorithm into four parts in total: (1). Discriminator; (2). Generator; (3). Inverter; (4). Search Algorithm.
									</p>
									<p>
										We first train the discriminator, which corresponds to the dark blue box in the figure above.
										The discriminator takes a graph as input and returns a scalar between -1 and 1.
										During the training of the discriminator, we freeze the parameters of all other models.
										The discriminator is trained to maximize its output for graphs from the dataset and to minimize its output for graphs generated by GraphRNN.
										Finally, to gauge how close the distribution of GraphRNN is to the true distribution, we take the difference between the expected value of the discriminator on the true dataset and the expected value of the discriminator on the generated graphs, which by the Kantorovich-Rubinstein duality is the Wasserstein distance.
									</p>
									<p>
										We then train the generator, which corresponds to the dark green box in the figure.
										To train the generator, we first freeze the parameters of all other models and then sample a batch of noise from a normal distribution on the latent space.
										This noise is passed to the generator to generate a batch of fake graphs. The generator is then trained to maximize the output of the (frozen) discriminator
										because this decreases the Wasserstein distance between the true distribution and the generator’s distribution.
									</p>
									<p>
										After that, we train the inverter, which corresponds to the red blocks from the image.
										A composition of a graph2vec model and a multi-layer perceptron, in that order,
										is what we call the inverter, whose domain is the space of graphs and whose codomain is the latent space of GraphRNN.
										The graph2vec model is pre-trained on the graphs produced by GraphRNN and on the graphs of the true distribution,
										and its weights are frozen hereafter. The inverter is trained to minimize the weighted sum of two distances:
										the standard Euclidean distance between an element in the latent space and the inverter’s output when given a graph generated from that element,
										and the Gromov-Wasserstein distance between a graph (either from the dataset or from GraphRNN) and GraphRNN’s output when given a latent element outputted by the inverter when the inverter’s argument is that graph.
									</p>
								</header>

								<header class="style4">
									<h4>What We Have Currently</h4>
									<div class="image section">
										<span class="image main"><a href="" target="_blank"><img src="images/Method Illustraion2.png" alt="" /></a></span>
										<p>Figure 5.2: Current structure of our attack generation procedure. </p>
									</div>
									<p>
										We were only able to train the discriminator and the generator due to time constraints. The discriminator is a weakened form of the originally planned one: instead of first passing graphs to a pre-trained graph2vec model, we merely pad flattened adjacency matrices to a common size and then feed it to a multi-layer perceptron.
									</p>
								</header>
							</div>
						</section>

						<!-- Here is our results, probably not empirical results, but the training procedure -->
						<section id="five">
							<div class="inner">
								<header class="major">
									<h4>Result of our Investigation</h4>
								</header>
								<h5>1. New Training Procedure for GraphRNN</h5>
								<p>
									Originally, GraphRNN is trained by comparing the generated padded adjacency vector with the sequenced original adjacency matrix using binary cross entropy loss.
									This isn't ideal for the WGAN training procedure, where the generator needs to be trained by the discriminator's output, which is no longer an adjacency matrix.
								</p>

								<p>
									For our new training process for the GraphRNN, we revert the output of node-level and edge-level RNN models to batches of generated adjacency matrices; then we pass these
									adjacency matrices into the discriminator network, whose output is used in our Wasserstein loss function. We will use this loss to update the node-level and edge-level RNN model parameters.
								</p>

								<!-- This is prob going to be a discussion of how our three losses are not working -->
								<h5>2. Observation on the three Losses</h5>
								<p>
									With our proposed WGAN training regime, we trained the generator, the discriminator, and the inverter, all in one training loop.
									We recorded the loss of each of the models every epoch.
								</p>
								<div class="image section">
									<span class="image main"><a href="https://arxiv.org/pdf/1710.11342.pdf" target="_blank"><img src="images/Graph Embedding.png" alt="" /></a></span>
									<p>Figure 6: Comparison of Graph2Vec Embedding between Real Graphs and Generated Graphs. </p>
								</div>
								<p>
									We found that as the discriminator loss improves overtime, the generator loss and inverter loss quickly hits a threshold and cannot
									improve further beyond this point.

									This can be an indication that the new training regime isn't suitable for GraphRNN training, and the quality of our generated graphs is not good enough
									support the training of the discriminator and inverter.
								</p>
							</div>
						</section>

						<!-- Here we write about what went wrong, and some ideas to make them right -->
						<section id="six">
							<div class="inner">
								<header class="major">
									<h4>Challenges</h4>
								</header>
								<h5>1. Previous work and time constraint</h5>
								<p>
									There isn’t much research done in generating latent representations of adversarial examples,
									especially not for the task of attacking graph classifiers, so we had to implement our ideas from the ground up.
									Also, the time frame for this project is 10 weeks, which is very little time to fully develop this framework and run experiments.
								</p>

								<h5>2. GraphRNN output during training is difficult to interpret and process by the discriminator </h5>
								<p>
									The output of GraphRNN is a graph of almost arbitrary size.
									However, in practice, the GraphRNN implementation returned batched and packed versions of the original graph adjacency vector,
									which made every version of the discriminator fail to compute.
								</p>

								<h5>3. Lack of networkx integration in PyTorch/PyTorch Geometric</h5>

								<p>
									The procedure of computing high-quality graph features (e.g. clustering coefficient) is too complex for us to implement with solely PyTorch functions,
									so we relied on NetworkX functions to do these computations. This became an issue during the training of GraphRNN because NetworkX functions are incompatible with the way PyTorch computes gradients,
									which means that gradient descent cannot happen. We resorted to using a simple multi-layer perceptron as our discriminator, which takes as input flattened and padded adjacency matrices,
									but flattening and padding adjacency matrix loses structural information.
								</p>

								<header class="major">
									<h4>Future Plan</h4>
								</header>
								<p>If we continue this research project, we want to do the following:</p>
								<h5>1. Improve implementation of GraphRNN</h5>
								<p>
									Due to a lack of time, we did not thoroughly investigate the GraphRNN implementation and treated the model as a black box.
									We encountered many issues related to loading the data, model training, and graph generation using the official implementation.
									As we were troubleshooting the model, we noticed that the implementation is incompatible with downstream processing of GraphRNN outputs.
									We would have liked more time to edit GraphRNN’s implementation to be more compatible with other tasks.
								</p>
								<h5>2. Use a graph classifier as our discriminator instead of a simple multi-layer perceptron</h5>
								<p>
									Currently, the discriminator is a simple MLP that takes in flattened and padded adjacency matrices.
									The discriminator is trained by the negative Wasserstein distance between the generated distribution and the real data distribution.
									However, because reshaping adjacency matrices caused a loss of structural information, we would have preferred to use a model that could process a graph without reshaping operations.
								</p>
								<p>
									After evaluating the result of the current setup, we think a simple MLP can't take into account specific graph properties, and thus cannot accurately discern between the two distributions.
									We propose to substitute this MLP with a Graph Attention Machine (GAM) classifier, because GAM can utilize the Weisfeiler-Lehman kernel to capture graph features to discern between the real and the generated distributions.
									Ideally, this will provide high quality loss for the discriminator training and the generator training.
								</p>
							</div>
						</section>



						<section id="eight">
							<div class="inner">
								<header class="major">
									<h4>Reference</h4>
								</header>
								<ol>
									<li>GraphRNN: <a href="https://arxiv.org/abs/1802.08773" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/snap-stanford/GraphRNN">Original Implementation</a></li>
									<li>Attack on GNN with Bayesian Optimization: <a href="https://arxiv.org/abs/2111.02842" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/xingchenwan/grabnel">Original Implementation</a></li>
									<li>Wasserstein GAN: <a href="https://arxiv.org/abs/1701.07875" target="_blank">Paper</a>, <a target="_blank" href="https://github.com/martinarjovsky/WassersteinGAN">Original Implementation</a></li>
									<li>Graph Attention Model: <a href="http://ryanrossi.com/pubs/KDD18-graph-attention-model.pdf">Paper</a>, <a target="_blank" href="https://github.com/benedekrozemberczki/GAM/tree/03e13f01a59dba22c9c789dfa11ce84b614c0be0">Original Implementation</a></li>
								</ol>
							</div>
						</section>
					</div>

				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<!-- TODO: Replace with actual Email and LinkedIn links -->
										<h3>Winston Yu</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
										<h3>Jianming Geng</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
										<h3>Barry Xue</h3>
										<a href="#">Email</a>
										<a href="#">LinkedIn</a>
									</div>
								</section>
							</section>
							<section class="split">
								<div>
									<p> Project GitHub Page</p>
									<a href="https://github.com/Barry0121/graph-neural-net-benchmark" target="_blank" rel="noopener noreferrer" class="icon brands alt fa-github"><span class="label">Project GitHub Page</span></a>
								</div>
							</section>
						</div>
					</section>


				<!-- Footer -->
					<!-- <footer id="footer">
						<div class="inner">
							<ul class="icons">

							</ul>
						</div>
					</footer> -->

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>